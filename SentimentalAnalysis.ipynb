{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee57a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Aisha Nawaz \n",
    "\n",
    "# Sentiment Analysis on Social Media Comments\n",
    "# Client Name: SocialBuzz Inc. [Fictional]\n",
    "# Company Name: TextInsight AI [Fictional]\n",
    "\n",
    "# Description: SocialBuzz Inc. is a social media marketing agency looking to gauge public sentiment \n",
    "# towards their clients' brands. They have approached TextInsight AI to develop a sentiment analysis \n",
    "# model capable of analyzing social media comments and identifying whether the sentiment towards a brand\n",
    "# or product is positive, negative, or neutral.\n",
    "\n",
    "# Dataset: Sentiment Analysis for Social Media Comments (Online Dataset)\n",
    "# For this project, you can use publicly available datasets from platforms like Kaggle, Twitter API,\n",
    "# or other social media platforms that provide labeled data for sentiment analysis.\n",
    "\n",
    "# Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a0442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...      -1.0\n",
       "1  talk all the nonsense and continue all the dra...       0.0\n",
       "2  what did just say vote for modi  welcome bjp t...       1.0\n",
       "3  asking his supporters prefix chowkidar their n...       1.0\n",
       "4  answer who among these the most powerful world...       1.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1)  Data Collection: Acquire a dataset containing social media comments along with their sentiment\n",
    "# labels (positive, negative, or neutral).\n",
    "\n",
    "# I OBTAINED DATASET FROM :https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('datasetw5d4.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea66d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 162980 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162976 non-null  object \n",
      " 1   category    162973 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cce2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Data Cleaning: Preprocess the text data by removing special characters, stop words, and performing tokenization.\n",
    "dataset.isnull().sum()        #Finding count of null values\n",
    "dataset.dropna(inplace=True) #Dropping null values found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1375add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to keep the cleaned data seperate so I am copying into new dataset for further steps\n",
    "datanew=dataset.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55a3a8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>Tokenised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>456 crores paid neerav modi recovered congress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>dear rss terrorist payal gawar modi killing 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cover interaction forum left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>big project came india modi dream project happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ever listen like gurukul discipline maintained...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category  \\\n",
       "162975  why these 456 crores paid neerav modi not reco...      -1.0   \n",
       "162976  dear rss terrorist payal gawar what about modi...      -1.0   \n",
       "162977  did you cover her interaction forum where she ...       0.0   \n",
       "162978  there big project came into india modi dream p...       0.0   \n",
       "162979  have you ever listen about like gurukul where ...       1.0   \n",
       "\n",
       "                                                Tokenised  \n",
       "162975  456 crores paid neerav modi recovered congress...  \n",
       "162976  dear rss terrorist payal gawar modi killing 10...  \n",
       "162977                       cover interaction forum left  \n",
       "162978  big project came india modi dream project happ...  \n",
       "162979  ever listen like gurukul discipline maintained...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words , Special characters & Performing tokenization using nltk built-in library\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def cleaner(sentences):\n",
    "    sentences=str(sentences) #Converting input to string type\n",
    "    \n",
    "    # Removing stop words first:\n",
    "    stopWords = set(stopwords.words('english'))  #Obtaining all stop words in english language\n",
    "    inputWordTokens = word_tokenize(sentences)   #Tokenzing the input sentence passed\n",
    "    \n",
    "    # Converting words to lowercase to check if it is present in list of stopwords\n",
    "    CleanedSentenceV1 = [word.lower() for word in inputWordTokens if word.lower() not in stopWords]\n",
    "      \n",
    "    # Removing special characters\n",
    "    translationTable = str.maketrans('', '', string.punctuation) #obtaining translation table to help remove special chars\n",
    "    \n",
    "    results=[]\n",
    "    for word in CleanedSentenceV1:\n",
    "        translated=word.translate(translationTable) ##Applying the translation table to each word in input sentence \n",
    "        if(translated):\n",
    "            results.append(translated) #Appending it to result list (if there is anything left)\n",
    "            \n",
    "            \n",
    "    results = ' '.join(results) #Converting the result list into string form to return\n",
    "    return results                  \n",
    "       \n",
    "#Creating a new column  'tokenised' to save the newly cleaned text versions\n",
    "datanew['Tokenised']=datanew['clean_text'].apply(cleaner)\n",
    "datanew.tail() #To visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ee473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:    (0, 105462)\t0.13205919442781192\n",
      "  (0, 94728)\t0.2597606140610556\n",
      "  (0, 93782)\t0.1471128844177335\n",
      "  (0, 91057)\t0.34089487583722683\n",
      "  (0, 80394)\t0.3455799718157834\n",
      "  (0, 77499)\t0.29032480843432995\n",
      "  (0, 76893)\t0.18242154961866305\n",
      "  (0, 62445)\t0.0366348236984051\n",
      "  (0, 61601)\t0.2073981288388287\n",
      "  (0, 60281)\t0.23809359712369188\n",
      "  (0, 51950)\t0.22293922558863355\n",
      "  (0, 51322)\t0.17011977351496282\n",
      "  (0, 40506)\t0.13783853251609857\n",
      "  (0, 40478)\t0.2109793533817779\n",
      "  (0, 39378)\t0.13830622626613456\n",
      "  (0, 34686)\t0.22172681719264056\n",
      "  (0, 34621)\t0.27640487683701465\n",
      "  (0, 29330)\t0.22374422023461862\n",
      "  (0, 17899)\t0.19868875839963557\n",
      "  (0, 13679)\t0.25062492796726993  Y:  -1.0\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature Extraction: Use techniques like TF-IDF or word embeddings to convert text data into numerical representations.\n",
    "\n",
    "#NOTE: I AM USING THE TECHNIQUE \"Frequency-Inverse Document Frequency\":\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib \n",
    "\n",
    "def verctorizeIT(data): \n",
    "    vectorizer = TfidfVectorizer() #intializing it\n",
    "    \n",
    "    #Training \n",
    "    vectorizer.fit(data)   \n",
    "    \n",
    "    # Saving trained vectorizer for later use\n",
    "    joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "    \n",
    "    #Converting text into numerical representations & returning it\n",
    "    return vectorizer.transform(data)\n",
    "   \n",
    "vectors_X=verctorizeIT(datanew['Tokenised'])           #This is x-variable\n",
    "variable_Y=datanew['category']                         #This is y-variable\n",
    "\n",
    "print(\"X: \",vectors_X[0],\" Y: \",variable_Y[0]) #Printing first values to see variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3484559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SentimentAnalysisModel_w5d4.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4)  Model Selection: Choose a machine learning or deep learning model (e.g., Naive Bayes, LSTM) for sentiment classification.\n",
    "# Tools: Python, Jupyter Notebook, Pandas, Matplotlib, NLTK, Scikit-learn, TensorFlow/Keras\n",
    "\n",
    "#NOTE: I AM USING Naive Bayes FOR THIS PART.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "#Splitting the datset into train & test: 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors_X, variable_Y, test_size=0.2, random_state=42) \n",
    "\n",
    "#Initializing NB classifier\n",
    "classifier = MultinomialNB()\n",
    "#Training it\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Saving classifier for later use\n",
    "joblib.dump(classifier, \"SentimentAnalysisModel_w5d4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6fb57ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 97980)\t0.4271501673145705\n",
      "  (0, 67001)\t0.24272965826364015\n",
      "  (0, 66011)\t0.20672158148764047\n",
      "  (0, 62445)\t0.0570055059880635\n",
      "  (0, 36819)\t0.4703088724272155\n",
      "  (0, 18452)\t0.5203898831287828\n",
      "  (0, 10813)\t0.3948873289821749\n",
      "  (0, 6044)\t0.2543806971045976\n",
      "  (1, 92837)\t0.3554917045584327\n",
      "  (1, 72642)\t0.10348589925964967\n",
      "  (1, 65836)\t0.10648779544621347\n",
      "  (1, 64979)\t0.2415662206115273\n",
      "  (1, 62445)\t0.03387738327757818\n",
      "  (1, 59465)\t0.22893386820471173\n",
      "  (1, 51589)\t0.20599612032913947\n",
      "  (1, 45087)\t0.20559296045786132\n",
      "  (1, 35152)\t0.6714901320521551\n",
      "  (1, 32161)\t0.22893386820471173\n",
      "  (1, 30257)\t0.23176081898361262\n",
      "  (1, 23533)\t0.10646183577298962\n",
      "  (1, 15657)\t0.10275090020279957\n",
      "  (1, 6280)\t0.19438417349412632\n",
      "  (1, 5465)\t0.19560495773740746\n",
      "  (2, 105761)\t0.18084217033037123\n",
      "  (2, 105462)\t0.12191404072905129\n",
      "  :\t:\n",
      "  (3, 9910)\t0.19602719481078243\n",
      "  (3, 5080)\t0.20394456326594848\n",
      "  (3, 4810)\t0.17955605692283771\n",
      "  (4, 105462)\t0.1121937365216682\n",
      "  (4, 98681)\t0.1521595712901906\n",
      "  (4, 97258)\t0.15554014324105225\n",
      "  (4, 87831)\t0.17975874254250462\n",
      "  (4, 85089)\t0.16755234787726822\n",
      "  (4, 79399)\t0.3084564498486887\n",
      "  (4, 72347)\t0.30253050001378884\n",
      "  (4, 68536)\t0.21769300740453398\n",
      "  (4, 67375)\t0.2642295517985293\n",
      "  (4, 66961)\t0.12584575417062288\n",
      "  (4, 65663)\t0.14310036525628364\n",
      "  (4, 62445)\t0.03112390451377017\n",
      "  (4, 43545)\t0.1501862379505306\n",
      "  (4, 38726)\t0.30253050001378884\n",
      "  (4, 38320)\t0.26851316155463245\n",
      "  (4, 28660)\t0.14306103174505944\n",
      "  (4, 26723)\t0.15483688812244883\n",
      "  (4, 24815)\t0.1094662847319803\n",
      "  (4, 24792)\t0.29359491759022943\n",
      "  (4, 23413)\t0.2532334053644232\n",
      "  (4, 10392)\t0.31825049452477033\n",
      "  (4, 9588)\t0.21049209254001297 Prediction:  [1. 1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test data now:\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(X_test[0:5],\"Prediction: \",y_pred[0:5]) #Seeing first 5 predictions raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bdb5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ > EVALUATING MODEL'S PERFORMANCE < ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Accuracy--->> 0.576 %\n",
      "Precision--->> 0.733 %\n",
      "Recall--->> 0.576 %\n",
      "F1-score--->> 0.514 %\n"
     ]
    }
   ],
   "source": [
    "# 5) Model Evaluation: Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ > EVALUATING MODEL\\'S PERFORMANCE < ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "print(\"Accuracy--->>\", round(accuracy_score(y_test, y_pred),3),\"%\")\n",
    "print(\"Precision--->>\", round(precision_score(y_test, y_pred,average='weighted'),3),\"%\")\n",
    "print(\"Recall--->>\", round(recall_score(y_test, y_pred,average='weighted'),3),\"%\")\n",
    "print(\"F1-score--->>\", round(f1_score(y_test, y_pred,average='weighted'),3),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58793e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:8000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [28/Jul/2023 19:43:15] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Jul/2023 19:43:55] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# 6) Deployment: Deploy the sentiment analysis model as an API that SocialBuzz Inc.\n",
    "# can use to analyze social media comments and track brand sentiment.\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def getResults(predictions): #Basically further clarifies the model's predictions\n",
    "    #Prediction of 0 Indicates  Neutral , 1 Indicates Postive,-1 Indicates Negative \n",
    "    clearResults=[]\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if (pred==0):\n",
    "            clearResults.append('Neutral')\n",
    "        elif (pred==1):\n",
    "            clearResults.append('Positive')\n",
    "        elif (pred==-1):\n",
    "            clearResults.append('Negative')\n",
    "            \n",
    "    return clearResults\n",
    "            \n",
    "    \n",
    "# Initializing Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Loading the model & vectorizer saved above\n",
    "model = joblib.load(\"SentimentAnalysisModel_w5d4.pkl\")\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def sentimentAnalysis():\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            #Getting comments from html & splitting them \n",
    "            rawComments=request.form['comments'].split(\",\") \n",
    "            \n",
    "            comments = [comment.strip() for comment in rawComments]  #Removing spaces from each comment\n",
    "            \n",
    "            #Calling the cleaner function i made above to preprocess each comment in the list\n",
    "            preprocessedComments = [cleaner(comment) for comment in comments]\n",
    "            \n",
    "            #Using the trained vectorizer I loaded above to vectorize each preprocessed comment in the list\n",
    "            vectors=vectorizer.transform(preprocessedComments)\n",
    "\n",
    "            #Making predictions\n",
    "            predictions = model.predict(vectors)\n",
    "           \n",
    "            sentimentLabels = getResults(predictions) #Clarifying results \n",
    "            \n",
    "            return render_template('model.html', sentiments=sentimentLabels) #Sending response back to web/html page\n",
    "\n",
    "        except Exception as e:\n",
    "            return jsonify({'error': str(e)}), 400\n",
    "\n",
    "    else:\n",
    "        return render_template('model.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=8000,use_reloader=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
